# FIXED System - Complete Setup & Usage Guide

## ‚úÖ What Was Fixed

### 1. **Removed Gemini Dependency**
- ‚úÖ Removed `google-generativeai` from requirements
- ‚úÖ Removed Gemini agent initialization from `app.py`
- ‚úÖ Updated frontend HTML to reference TinyLlama instead of Gemini

### 2. **Improved HuggingFace Agent**
- ‚úÖ **Better prompts**: Added context-aware and emotion-aware system messages
- ‚úÖ **Conversation history**: Now includes last 3 exchanges for continuity
- ‚úÖ **Improved generation params**: Optimized temperature, top_k, top_p for better quality
- ‚úÖ **Better post-processing**: Removes artifacts, limits to 3 sentences, validates output
- ‚úÖ **Smarter fallbacks**: Uses context-appropriate fallback responses

### 3. **Frontend Updates**
- ‚úÖ Updated info panel to reference TinyLlama (100% free)
- ‚úÖ All agent switching UI remains functional
- ‚úÖ Cost estimate now shows $0.00

---

## üöÄ How to Start the System

### Option 1: Quick Start (Recommended)
```bash
# Make sure you're in the project directory
cd /Users/kanishkraghavendra/Documents/Project/Dynamic_NPC_Dialogue_Generation

# Run the setup script
./start.sh

# If that doesn't work, run:
bash start.sh

# Then start the server:
source venv/bin/activate
python app.py
```

### Option 2: Manual Setup
```bash
# 1. Activate virtual environment
source venv/bin/activate

# 2. Install dependencies (if not already installed)
pip install flask flask-cors python-dotenv pandas transformers torch sentence-transformers chromadb

# 3. Start the server
python app.py
```

### Option 3: Using Python 3 directly
```bash
python3 app.py
```

---

## üåê Accessing the Web Interface

Once the server starts, you'll see:
```
üåê Server running at http://localhost:3000
```

**Open your browser and go to:** `http://localhost:3000`

---

## üí¨ How to Have Proper Conversations

### The System Now Includes:

#### 1. **Conversation History** (Automatic)
- Last 3 exchanges are automatically included in the prompt
- Provides context for follow-up questions
- Makes conversations feel more natural

**Example:**
```
You: "Hello Nick"
Nick: "Hello. What can I do for you?"

You: "I need help with a case"
Nick: "A case, huh? What kind of trouble you got yourself into?"
         ‚Üë Nick remembers you just greeted him
```

#### 2. **Context-Aware Responses**
Select the appropriate context dropdown:
- **Investigation**: Detective work, clues, cases
- **Combat**: Dangerous situations, threats
- **Casual**: Normal conversation
- **Emotional**: Deep feelings, personal topics
- **Greeting**: First meetings

#### 3. **Emotion Control**
Select emotion to influence Nick's tone:
- **Neutral**: Standard detective tone
- **Amused**: Dry humor, sarcastic
- **Stern**: Serious, no-nonsense
- **Concerned**: Worried, caring
- **Irritated**: Impatient, annoyed
- **Somber**: Reflective, serious

#### 4. **Vector Examples** (Include Examples checkbox)
- When checked: Retrieves similar Nick Valentine quotes from database
- Makes responses more authentic and in-character
- Slightly slower but much better quality

---

## üé≠ Example Conversations

### Example 1: Investigation Context
```
Context: Investigation
Emotion: Neutral

You: "I found some blood at the crime scene"
Nick: "Blood at the scene? That's a start. Now we need to figure out who it belongs to."

You: "Could it be the victim's?"
Nick: "Could be. But I've seen cases where the killer left their own calling card."
```

### Example 2: Casual Context with Emotion
```
Context: Casual
Emotion: Amused

You: "How do you like Diamond City?"
Nick: "It's got character, I'll give it that. Nothing like the old days, but it'll do."

You: "What were the old days like?"
Nick: "Back then, we had real detectives, real cases. Now? Just synths and secrets."
```

### Example 3: Emotional Context
```
Context: Emotional
Emotion: Somber

You: "Do you ever feel alone?"
Nick: "Sometimes. Being a synth in a human world... you learn to live with it."

You: "That must be hard"
Nick: "It is what it is. I've got a job to do, and that keeps me going."
```

---

## üîß Generation Parameters (Already Optimized)

The system now uses these optimal settings:

```python
max_new_tokens = 50       # Allows complete thoughts
temperature = 0.85        # Balanced creativity/focus
top_p = 0.92             # Nucleus sampling
top_k = 40               # Top-k filtering
repetition_penalty = 1.2  # Prevents loops
```

---

## üéØ Tips for Best Results

### DO:
‚úÖ **Use appropriate context** - Set Investigation for detective work, Casual for small talk
‚úÖ **Keep "Include Examples" checked** - Much better quality with vector retrieval
‚úÖ **Ask follow-up questions** - System remembers last 3 exchanges
‚úÖ **Use clear, direct questions** - "What do you think of Piper?" vs "Tell me stuff"
‚úÖ **Select emotions** - Helps guide the tone of Nick's response

### DON'T:
‚ùå **Ask very long complex questions** - Keep queries focused (1-2 sentences)
‚ùå **Expect perfect responses** - TinyLlama is small, not GPT-4
‚ùå **Switch context mid-conversation** - Causes confusion
‚ùå **Uncheck "Include Examples"** - Quality drops significantly

---

## üêõ Troubleshooting

### Problem: Server won't start
```bash
# Solution 1: Check if Flask is installed
source venv/bin/activate
pip list | grep -i flask

# Solution 2: Reinstall dependencies
pip install --force-reinstall flask flask-cors

# Solution 3: Check Python version
python --version  # Should be 3.8+
```

### Problem: "No agents initialized"
```bash
# This means the HuggingFace agent failed to load
# Check the terminal output for the actual error

# Common fix: Install transformers
pip install transformers torch
```

### Problem: Responses are gibberish
```
# Solution 1: Make sure "Include Examples" is checked
# Solution 2: Select appropriate context
# Solution 3: Restart server (model may not have loaded correctly)
```

### Problem: Very slow responses
```
# This is normal on CPU
# TinyLlama takes 5-10 seconds per response on CPU
# Expect longer on first request (model loading)
```

### Problem: "Model not found" error
```bash
# The model will download on first run (~2.2GB)
# Make sure you have:
# 1. Internet connection
# 2. Enough disk space
# 3. Patience (first download takes 5-10 minutes)
```

---

## üìä What Each File Does

### Backend:
- **`app.py`** - Flask server, routes, API endpoints
- **`agents/huggingface_agent.py`** - TinyLlama generation (FIXED)
- **`agents/agent_manager.py`** - Manages agents and conversation history
- **`agents/base_agent.py`** - Base class for all agents
- **`dialogue_processor.py`** - CSV parser for Nick's dialogue examples
- **`vector_store.py`** - ChromaDB vector search (optional)

### Frontend:
- **`public/index.html`** - Main page structure (FIXED)
- **`public/app.js`** - JavaScript for UI and API calls
- **`public/styles.css`** - Styling

### Data:
- **`data/nick_valentine_dialogue.csv`** - Nick's dialogue database

---

## üéì For Your TA Demo

### What to Show:

1. **Start the server**
   ```bash
   python app.py
   ```

2. **Open browser** to `http://localhost:3000`

3. **Demonstrate conversation flow:**
   - Set Context: Investigation
   - Emotion: Neutral
   - Ask: "I need help with a murder case"
   - Show Nick's response
   - Ask follow-up: "What should I look for?"
   - Show how Nick remembers the previous exchange

4. **Show different contexts:**
   - Switch to Casual
   - Ask: "How are you today?"
   - Show different tone

5. **Highlight features:**
   - ‚úÖ 100% free (no API costs)
   - ‚úÖ Conversation memory (last 3 exchanges)
   - ‚úÖ Context-aware responses
   - ‚úÖ Vector search for authentic quotes
   - ‚úÖ Real-time generation

### What to Emphasize:
- **No Gemini safety blocks** - Removed paid API completely
- **Local generation** - TinyLlama runs on your machine
- **Conversation continuity** - System remembers context
- **Quality improvements** - Better prompts and post-processing

---

## üÜö Before vs. After Fixes

| Aspect | BEFORE (Gemini) | AFTER (TinyLlama) |
|--------|-----------------|-------------------|
| **Safety Blocks** | ‚ùå Frequent | ‚úÖ None |
| **Conversation Memory** | ‚ùå None | ‚úÖ Last 3 exchanges |
| **Context Awareness** | ‚ùå Basic | ‚úÖ Advanced |
| **Prompt Quality** | ‚ùå Oversimplified | ‚úÖ Rich with examples |
| **Cost** | ‚ö†Ô∏è Free tier (limited) | ‚úÖ $0 forever |
| **Response Quality** | ‚≠ê‚≠ê‚≠ê‚≠ê (when not blocked) | ‚≠ê‚≠ê‚≠ê (consistent) |
| **Reliability** | ‚ùå Unpredictable | ‚úÖ Consistent |

---

## üéâ Summary

**Your system is now:**
- ‚úÖ Working with TinyLlama (no Gemini)
- ‚úÖ Has proper conversation memory
- ‚úÖ Context-aware and emotion-aware
- ‚úÖ Better prompts and post-processing
- ‚úÖ 100% free with no API costs
- ‚úÖ Ready for demo

**To start using:**
```bash
source venv/bin/activate
python app.py
# Open http://localhost:3000
```

**For best results:**
- Keep "Include Examples" checked
- Use appropriate context
- Ask clear, focused questions
- Let it remember previous exchanges

üöÄ **Ready to go!**
